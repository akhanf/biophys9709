{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMrlV/TxOVGMJU5+7vxOjy8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akhanf/biophys9709/blob/2025/2025_Lecture_10B_Unet_Demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZPISU8EDaEVr"
      },
      "source": [
        "# Unet Demo\n",
        "\n",
        "- 2D images and segmentations\n",
        "- ImageDataGenerators to get the images and segmentations\n",
        "- Will use padding to avoid size discrepancy at input/output\n",
        "- Explore how a custom loss can be defined and how this affects the result\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZTwnzRo640f1"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c35IoKW5tBgL"
      },
      "source": [
        "## Get Data\n",
        "\n",
        "I created a brain dataset for this example.\n",
        "\n",
        "Let's load it up to see what it looks like"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cIsTGNH-5LA0"
      },
      "source": [
        "# load and examine the data\n",
        "import os\n",
        "\n",
        "#download the zip file\n",
        "if not os.path.exists('brain_2d_seg_data.zip'):\n",
        "  !wget https://www.dropbox.com/s/3so8n63ast4dfcg/brain_2d_seg_data.zip\n",
        "  !unzip brain_2d_seg_data.zip > /dev/null\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5dm2VLvycBJ_"
      },
      "source": [
        "## Take a look at what we have downloaded"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Xb_TKoUhThn"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7NDqualhUUn"
      },
      "source": [
        "## Loading the dataset\n",
        "\n",
        "In previous versions of Keras we would instantiate an ImageDataGenerator, and use flow_from_directory() to read data whil;e performing augmentation.\n",
        "\n",
        "Now, we will instead use the Keras `image_dataset_from_directory()` function to read the dataset, then apply augmentation as needed.\n",
        "\n",
        "We wrap this function along with a data normalization step since we will apply it for images and masks in the training and test dataset."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to load datasets\n",
        "def load_image_dataset(directory, subset, image_size=(160, 160), batch_size=1, seed=1):\n",
        "    dataset = tf.keras.utils.image_dataset_from_directory(\n",
        "        directory,\n",
        "        labels=None,  # Since it's segmentation, we don't need labels\n",
        "        seed=seed,\n",
        "        image_size=image_size,\n",
        "        batch_size=batch_size,\n",
        "        color_mode=\"grayscale\",  # Ensure grayscale loading\n",
        "        shuffle=False  # Keep order intact, as we will shuffle later..\n",
        "    )\n",
        "    # Normalize images (rescale pixel values to [0,1])\n",
        "    return dataset.map(lambda x: x/255.0)\n",
        "\n",
        "# Load images and masks for training\n",
        "image_dataset_train = load_image_dataset(\"brain_2d_seg_data/training/images\", subset=\"training\")\n",
        "mask_dataset_train = load_image_dataset(\"brain_2d_seg_data/training/brain_masks\", subset=\"training\")\n",
        "\n",
        "# Load images and masks for testing\n",
        "image_dataset_test = load_image_dataset(\"brain_2d_seg_data/test/images\", subset=\"test\")\n",
        "mask_dataset_test = load_image_dataset(\"brain_2d_seg_data/test/brain_masks\", subset=\"test\")\n",
        "\n",
        "\n",
        "# Zip images and masks together\n",
        "train_dataset = tf.data.Dataset.zip((image_dataset_train, mask_dataset_train))\n",
        "test_dataset = tf.data.Dataset.zip((image_dataset_test, mask_dataset_test))\n"
      ],
      "metadata": {
        "id": "_qEevTTs5KG8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define augmentation function, which takes both image and mask:\n",
        "from tensorflow.keras.layers import RandomFlip, RandomRotation, RandomZoom, RandomTranslation, RandomBrightness\n",
        "\n",
        "\n",
        "augmentation_layer = tf.keras.Sequential([\n",
        "    RandomFlip(\"horizontal\"),\n",
        "    RandomRotation(0.1,fill_mode='constant',fill_value=0),\n",
        "])\n",
        "\n",
        "\n",
        "def augment_with_keras(image, mask):\n",
        "    \"\"\"Apply the same Keras augmentation to both image and mask.\"\"\"\n",
        "\n",
        "    # Stack image & mask to apply the same transformation\n",
        "    combined = tf.concat([image, mask], axis=-1)\n",
        "\n",
        "    # Apply augmentation\n",
        "    augmented = augmentation_layer(combined)\n",
        "\n",
        "    # Split image and mask back\n",
        "    augmented_image = augmented[..., 0:1]  # First channel is the image\n",
        "    augmented_mask = augmented[..., 1:2]  # Second channel is the mask\n",
        "\n",
        "    return augmented_image, augmented_mask\n",
        "\n"
      ],
      "metadata": {
        "id": "Fhl-5t2j797n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for img_batch, mask_batch in train_dataset.take(5):  # This should be original images\n",
        "\n",
        "    aug_img, aug_mask = augment_with_keras(img_batch, mask_batch)  # Apply augmentation manually\n",
        "\n",
        "    # Visualize\n",
        "    plt.figure()\n",
        "    plt.imshow(img_batch[0, :, :, 0], cmap=\"gray\")\n",
        "    plt.imshow(mask_batch[0, :, :, 0], alpha=0.5)\n",
        "    plt.colorbar()\n",
        "\n",
        "    plt.figure()\n",
        "    plt.imshow(aug_img[0, :, :, 0], cmap=\"gray\")\n",
        "    plt.imshow(aug_mask[0, :, :, 0], alpha=0.5)\n",
        "    plt.colorbar()\n"
      ],
      "metadata": {
        "id": "aM8GnXBF6sSO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#apply augmentation, shuffle, repeat..\n",
        "train_dataset = train_dataset.map(augment_with_keras)\n",
        "train_dataset = train_dataset.shuffle(buffer_size=100)\n",
        "train_dataset = train_dataset.repeat()"
      ],
      "metadata": {
        "id": "4sDqAwCgBnf1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m1GNG8dx3UbM"
      },
      "source": [
        "### Use the Dice metric to evaluate performance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bcQxXqxwrXkU"
      },
      "source": [
        "def dice_metric(y_true, y_pred):\n",
        "     y_true_f = tf.reshape(y_true,[-1])\n",
        "     y_pred_f = tf.reshape(y_pred,[-1])\n",
        "     intersection = tf.reduce_sum(y_true_f * y_pred_f)\n",
        "     score = (2. * intersection + 1.) / (tf.reduce_sum(y_true_f) + tf.reduce_sum(y_pred_f) + 1.)\n",
        "\n",
        "     return score\n",
        "\n",
        "def dice_loss(y_true, y_pred):\n",
        "    return (1 - dice_metric(y_true, y_pred))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQ8imoOJe-qS"
      },
      "source": [
        "## Create our model\n",
        "\n",
        "Use:\n",
        "- Max-pooling & up-convolutions\n",
        "- 16 deep in first conv layer\n",
        "- 2 convolutions in each stage\n",
        "- 4 skip connections\n",
        "- padding to get same size outputs\n",
        "\n",
        "\n",
        "Set each layer to `x`, to make it easier to copy-paste and re-arrange things.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BYG4Z_zz7xrQ"
      },
      "source": [
        "img_shape = (160,160,1)\n",
        "input_layer = keras.layers.Input(img_shape)\n",
        "\n",
        "x = keras.layers.Conv2D(16,(3,3), padding='same',activation='relu')(input_layer)\n",
        "x = keras.layers.Conv2D(16,(3,3), padding='same',activation='relu')(x)\n",
        "out_layer1 = x\n",
        "x = keras.layers.MaxPooling2D((2,2))(x)\n",
        "\n",
        "x = keras.layers.Conv2D(32,(3,3), padding='same',activation='relu')(x)\n",
        "x = keras.layers.Conv2D(32,(3,3), padding='same',activation='relu')(x)\n",
        "out_layer2 = x\n",
        "x = keras.layers.MaxPooling2D((2,2))(x)\n",
        "\n",
        "x = keras.layers.Conv2D(64,(3,3), padding='same',activation='relu')(x)\n",
        "x = keras.layers.Conv2D(64,(3,3), padding='same',activation='relu')(x)\n",
        "out_layer3 = x\n",
        "x = keras.layers.MaxPooling2D((2,2))(x)\n",
        "\n",
        "\n",
        "x = keras.layers.Conv2D(128,(3,3), padding='same',activation='relu')(x)\n",
        "x = keras.layers.Conv2D(128,(3,3), padding='same',activation='relu')(x)\n",
        "out_layer4 = x\n",
        "x = keras.layers.MaxPooling2D((2,2))(x)\n",
        "\n",
        "x = keras.layers.Conv2D(256,(3,3), padding='same',activation='relu')(x)\n",
        "x = keras.layers.Conv2D(256,(3,3), padding='same',activation='relu')(x)\n",
        "\n",
        "x = keras.layers.UpSampling2D((2,2))(x)\n",
        "x = keras.layers.Conv2D(128,(2,2), padding='same',activation='relu')(x)\n",
        "\n",
        "x = keras.layers.Concatenate(axis=3)([out_layer4,x])\n",
        "x = keras.layers.Conv2D(128,(3,3), padding='same',activation='relu')(x)\n",
        "x = keras.layers.Conv2D(128,(3,3), padding='same',activation='relu')(x)\n",
        "\n",
        "\n",
        "x = keras.layers.UpSampling2D((2,2))(x)\n",
        "x = keras.layers.Conv2D(64,(2,2), padding='same',activation='relu')(x)\n",
        "\n",
        "x = keras.layers.Concatenate(axis=3)([out_layer3,x])\n",
        "x = keras.layers.Conv2D(64,(3,3), padding='same',activation='relu')(x)\n",
        "x = keras.layers.Conv2D(64,(3,3), padding='same',activation='relu')(x)\n",
        "\n",
        "\n",
        "x = keras.layers.UpSampling2D((2,2))(x)\n",
        "x = keras.layers.Conv2D(32,(2,2), padding='same',activation='relu')(x)\n",
        "\n",
        "\n",
        "x = keras.layers.Concatenate(axis=3)([out_layer2,x])\n",
        "x = keras.layers.Conv2D(32,(3,3), padding='same',activation='relu')(x)\n",
        "x = keras.layers.Conv2D(32,(3,3), padding='same',activation='relu')(x)\n",
        "\n",
        "x = keras.layers.UpSampling2D((2,2))(x)\n",
        "x = keras.layers.Conv2D(16,(2,2), padding='same',activation='relu')(x)\n",
        "\n",
        "x = keras.layers.Concatenate(axis=3)([out_layer1,x])\n",
        "x = keras.layers.Conv2D(16,(3,3), padding='same',activation='relu')(x)\n",
        "x = keras.layers.Conv2D(16,(3,3), padding='same',activation='relu')(x)\n",
        "\n",
        "#1x1 conv with sigmoid to get binary classification at each pixel\n",
        "x = keras.layers.Conv2D(1,(1,1), padding='same',activation='sigmoid')(x)\n",
        "\n",
        "model = keras.Model(input_layer, x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7gvXGLJiOTz"
      },
      "source": [
        "###  Compile and visualize it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ir9evaNgVz9e"
      },
      "source": [
        "\n",
        "# Compile the model\n",
        "optimizer = keras.optimizers.Adam()\n",
        "loss = ['binary_crossentropy',dice_loss]\n",
        "metrics = ['binary_accuracy',dice_metric]\n",
        "\n",
        "\n",
        "model.compile(loss=loss,\n",
        "              optimizer=optimizer,\n",
        "              metrics=metrics)\n",
        "\n",
        "# What does the finished model look like?\n",
        "model.summary()\n",
        "keras.utils.plot_model(model, show_shapes=True, rankdir='TD')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZmGWhW-6Nrj"
      },
      "source": [
        "## Fit the model\n",
        "\n",
        "The fit() function takes x and y -- we want to pass the image and mask  correspondingly.\n",
        "\n",
        "But since we are using generators, we need the output of the generator to be: `(image, mask)`\n",
        "\n",
        "We can create this behaviour by using `zip()` to zip together the image and mask generators.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for x, y in train_dataset.take(1):\n",
        "    print(\"Image shape:\", x.shape)\n",
        "    print(\"Mask shape:\", y.shape)\n"
      ],
      "metadata": {
        "id": "x-BVNl5iH_iy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M6l5RnNl1V24"
      },
      "source": [
        "# combine generators into one which yields both image and masks\n",
        "#train_generator = zip(train_dataset, mask_generator_train)\n",
        "#test_generator = zip(image_generator_test, mask_generator_test)\n",
        "\n",
        "history = model.fit(train_dataset,\n",
        "                    steps_per_epoch=100,\n",
        "                    epochs=10)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8GLedPUlvhb"
      },
      "source": [
        "Plot the loss and metrics on training data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "khko21Ytt2MT"
      },
      "source": [
        "#plot loss and metrics\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "df = pd.DataFrame(history.history)\n",
        "sns.lineplot(data=df)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M3K7LyZbly4I"
      },
      "source": [
        "Evaluate the metrics on the test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SclHi0HHxjTG"
      },
      "source": [
        "#model.evaluate to get avg metric\n",
        "metrics = model.evaluate(test_dataset,steps=197,return_dict=True)\n",
        "print(metrics)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ktLINMt0Im8c"
      },
      "source": [
        "Let's take a look at some results for the test dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "XaztoulaCZd1"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AkSZZIVXEcWt"
      },
      "source": [
        "#plot some examples from the test set\n",
        "\n",
        "for sample,(image,mask) in enumerate(test_dataset):\n",
        "\n",
        "  predicted = model.predict(image)\n",
        "\n",
        "  plt.figure()\n",
        "\n",
        "  plt.subplot(1,2,1)\n",
        "  plt.imshow(np.squeeze(image),cmap='gray')\n",
        "  plt.imshow(np.squeeze(mask),alpha=0.5)\n",
        "\n",
        "  plt.subplot(1,2,2)\n",
        "  plt.imshow(np.squeeze(image),cmap='gray')\n",
        "  plt.imshow(np.squeeze(predicted),alpha=0.5)\n",
        "\n",
        "\n",
        "  if sample > 5:\n",
        "    break\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IRSr8wZWo4M8"
      },
      "source": [
        "## Now what?\n",
        "\n",
        "- Try segmenting the ventricles with this configuration\n",
        "\n",
        "- Try using a custom loss function\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qiFxE0ExXv-v"
      },
      "source": [
        "## Beyond this example\n",
        "\n",
        "### More than just binary labels?\n",
        "\n",
        "We took a shortcut here and used a single channel output and sigmoid to get our label. More generally if you have multiple labels, you will want to use a *one-hot* encoding analogous to what we did for multi-class classification, and use a soft-max activation.\n",
        "\n",
        " E.g. if we wanted to have our example data (brain and ventricle segmentation) set up in this way, we would need to have the following 3 channels:\n",
        " - background: 1 where neither brain nor ventricles are, 0 elsewhere\n",
        " - brain: 1 where brain is, 0 elsewhere (we have this already)\n",
        " - ventricles: 1 where ventricles are, 0 elsewhere (we have this already)\n",
        "\n",
        "So in this case you would need to create the background channel, e.g. via logical operations on the other channels.\n",
        "\n",
        "If you are starting with a single image with multiple labels on it, then you should be able to use Keras' built-in `to_categorical()` to achieve this.\n",
        "\n",
        "Note that if you have multiple labels, you would also need to account for this in any custom loss functions you create.\n",
        "\n",
        "### Validation split?\n",
        "We didn't use a validation split here for the sake of simplicity. You can achieve this by either putting your validation data in another directory and creating another Dataset, **or** you can use specify `subset='validation'` or `subset='training'` when defining the dataset to perform the split.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "b4AlIQ3PcFip"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}